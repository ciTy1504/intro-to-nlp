% !TEX root = ../../main.tex
% File: part2/chapters2/chap2_2.tex

\section{Kỹ thuật Đánh giá Thực tế}
\label{sec:practical_evaluation}

Việc theo dõi các metric đánh giá trong suốt quá trình huấn luyện là cực kỳ quan trọng. Nó không chỉ cho chúng ta biết mô hình đang hoạt động tốt đến đâu, mà còn giúp phát hiện sớm các vấn đề như quá khớp (overfitting) hoặc dưới khớp (underfitting).

`Trainer` API của thư viện `transformers` cung cấp một cách rất tiện lợi để thực hiện việc đánh giá định kỳ trên tập validation. Chìa khóa để làm việc này là cung cấp cho nó một hàm tùy chỉnh, thường được gọi là `compute\_metrics`.

\subsection{Hàm \texttt{compute\_metrics}: Cầu nối giữa Mô hình và Metric}
\label{ssec:compute_metrics_function}

\begin{itemize}
    \item \textbf{Mục đích:} Hàm `compute\_metrics` là một hàm Python mà bạn tự định nghĩa. `Trainer` sẽ gọi hàm này sau mỗi epoch (hoặc sau một số bước nhất định) trong quá trình huấn luyện.
    \item \textbf{Đầu vào:} Hàm này nhận vào một đối tượng `EvalPrediction`, là một tuple chứa hai mảng NumPy: `predictions` (các logits đầu ra từ mô hình) và `label\_ids` (các nhãn thật từ bộ dữ liệu).
    \item \textbf{Đầu ra:} Hàm phải trả về một dictionary, trong đó key là tên của metric (ví dụ: `"f1"`) và value là giá trị của metric đó.
\end{itemize}

\paragraph{Quy trình bên trong một hàm \texttt{compute\_metrics}}
Một hàm `compute\_metrics` điển hình sẽ thực hiện các bước sau:
\begin{enumerate}
    \item \textbf{Tải các metric cần thiết} từ thư viện `evaluate`.
    \item \textbf{Tiền xử lý đầu ra của mô hình:} Các `predictions` thường là các logits (các số thực thô). Chúng ta cần phải chuyển đổi chúng thành các dự đoán cuối cùng (ví dụ, bằng cách lấy `argmax` trên chiều cuối cùng) để có thể so sánh với các nhãn.
    \item \textbf{Tính toán các metric:} Sử dụng các đối tượng metric đã tải để tính toán điểm số bằng cách truyền vào các dự đoán đã xử lý và các nhãn thật.
    \item \textbf{Trả về kết quả:} Trả về một dictionary chứa các điểm số đã tính được.
\end{enumerate}

\begin{example}{Một hàm `compute\_metrics` cho bài toán phân loại chuỗi}{ex:compute_metrics_classification}
    Đây là một ví dụ hoàn chỉnh cho một bài toán phân loại nhị phân, sử dụng F1, Precision và Recall.
    \begin{minted}{python}
    import numpy as np
    import evaluate

    # Tải trước các metric để không phải tải lại mỗi lần gọi hàm
    f1_metric = evaluate.load("f1")
    precision_metric = evaluate.load("precision")
    recall_metric = evaluate.load("recall")

    def compute_metrics(eval_preds):
        # eval_preds là một tuple (predictions, label_ids)
        logits, labels = eval_preds
        
        # 1. Chuyển đổi logits thành dự đoán cuối cùng
        # Lấy chỉ số của logit lớn nhất trên chiều cuối cùng (-1)
        predictions = np.argmax(logits, axis=-1)
        
        # 2. Tính toán từng metric
        f1 = f1_metric.compute(predictions=predictions, references=labels)["f1"]
        precision = precision_metric.compute(predictions=predictions, references=labels)["precision"]
        recall = recall_metric.compute(predictions=predictions, references=labels)["recall"]
        
        # 3. Trả về một dictionary
        return {
            "f1": f1,
            "precision": precision,
            "recall": recall,
        }

    # Sau đó, hàm này sẽ được truyền vào đối tượng TrainingArguments:
    # training_args = TrainingArguments(..., evaluation_strategy="epoch")
    # trainer = Trainer(
    #     ...,
    #     args=training_args,
    #     compute_metrics=compute_metrics,
    # )
    # trainer.train()
    \end{minted}
\end{example}

\subsection{Đánh giá các Tác vụ Phức tạp hơn}
\label{ssec:complex_task_evaluation}

\subsubsection{Gán nhãn Chuỗi (NER) với `seqeval`}
\begin{itemize}
    \item \textbf{Vấn đề:} Đối với các tác vụ như Nhận dạng Thực thể Tên (NER), việc đánh giá không chỉ đơn giản là so sánh từng nhãn một. Chúng ta quan tâm đến việc mô hình có nhận dạng đúng \textbf{toàn bộ các thực thể} (bao gồm cả ranh giới và loại) hay không. Ví dụ, dự đoán `[B-PER, I-PER]` cho "Hồ Chí Minh" là đúng, nhưng dự đoán `[B-PER, B-LOC]` là sai hoàn toàn, mặc dù đúng được 1/2 nhãn.
    \item \textbf{Giải pháp: `seqeval`.} Đây là một thư viện Python (cũng được tích hợp trong `evaluate`) chuyên để đánh giá các tác vụ gán nhãn chuỗi. Nó tính toán Precision, Recall, và F1-score ở cấp độ thực thể (entity-level) theo các định dạng chuẩn như IOB2.
\end{itemize}

\begin{example}{Hàm \texttt{compute\_metrics} cho NER}{ex:compute_metrics_ner}
    \begin{minted}{python}
    import numpy as np
    import evaluate

    # Tải metric seqeval
    seqeval_metric = evaluate.load("seqeval")

    # Giả sử chúng ta có mapping từ ID sang nhãn
    label_names = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC']

    def compute_metrics_ner(eval_preds):
        logits, labels = eval_preds
        predictions = np.argmax(logits, axis=-1)

        # Chuyển đổi ID trở lại thành nhãn chuỗi
        # Bỏ qua các nhãn padding (thường có ID là -100)
        true_labels = [
            [label_names[l] for l in label if l != -100]
            for label in labels
        ]
        true_predictions = [
            [label_names[p] for p, l in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        
        # Tính toán metric
        results = seqeval_metric.compute(predictions=true_predictions, references=true_labels)
        
        # Trả về các metric chính
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "accuracy": results["overall_accuracy"],
        }
    \end{minted}
\end{example}

\subsubsection{Sinh Văn bản (Tóm tắt, Dịch máy) với ROUGE và BLEU}
\begin{itemize}
    \item \textbf{Vấn đề:} Các mô hình sinh văn bản (Decoder-Only, Encoder-Decoder) tạo ra các chuỗi văn bản có độ dài thay đổi. Việc đánh giá đòi hỏi phải so sánh các chuỗi này với các chuỗi tham khảo.
    \item \textbf{Thách thức trong `Trainer`:} `Trainer` không thể tự động thu thập các chuỗi văn bản được sinh ra từ nhiều GPU. Hơn nữa, quá trình decode (chuyển các token ID thành văn bản) cần phải được thực hiện.
    \item \textbf{Giải pháp:} `Trainer` có một phương thức đặc biệt là `predict`. Khi sử dụng `evaluation\_strategy` cùng với `predict\_with\_generate=True` trong `TrainingArguments`, `Trainer` sẽ tự động:
        \begin{enumerate}
            \item Chạy quá trình sinh văn bản (`.generate()`) trên tập validation.
            \item Thu thập (gather) các kết quả từ tất cả các thiết bị.
        \end{enumerate}
        Chúng ta chỉ cần thực hiện bước decode và tính toán metric trong hàm `compute\_metrics`.
\end{itemize}

\begin{example}{Hàm \texttt{compute\_metrics} cho Tóm tắt (sử dụng ROUGE)}{ex:compute_metrics_summarization}
    \begin{minted}{python}
    import numpy as np
    import evaluate
    from transformers import AutoTokenizer # Cần tokenizer để decode

    # Giả sử tokenizer đã được định nghĩa ở ngoài
    tokenizer = AutoTokenizer.from_pretrained("t5-small")
    rouge_metric = evaluate.load("rouge")

    def compute_metrics_summarization(eval_preds):
        predictions, labels = eval_preds

        # Decode các token ID thành văn bản
        # Bỏ qua các token padding
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        
        # Thay thế các token padding (-100) trong nhãn bằng pad_token_id
        # để chúng cũng có thể được decode đúng cách
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # Tính ROUGE score
        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)

        # Trích xuất các điểm số chính
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
        result["gen_len"] = np.mean(prediction_lens)
        
        return {k: round(v, 4) for k, v in result.items()}
    \end{minted}
\end{example}

Việc xây dựng một hàm `compute\_metrics` đúng đắn và phù hợp với tác vụ là một bước không thể thiếu để có được một quy trình huấn luyện và đánh giá đáng tin cậy.